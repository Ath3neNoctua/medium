---
title: "Rf"
author: "Söderström J."
date: "2023-06-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
rm(list=ls())
```

# libraries
```{r}
library(tidyverse, quietly = T)
library(tidymodels, quietly = T)
library(vip, quietly = T)
```

# introduction
Here is a framework I use for fitting random forests, explanations in comments. Enjoy! :-)


## Simulate correlated predictors.
```{r, initiate playgroudnd correlated}
# number of data points
n = 100
sigma = 2.5

set.seed(5)

# correlated, change runif to rnorm to get better behaviour
x1 <- runif(n)
x2 <- x1 + 2.0 * rnorm(n)
x3 <- rnorm(n)
x4 <- x3 + 0.5 * runif(n)
x5 <- x1 + runif(n) / 1.3
x6 <- x2 + 2.25 * runif(n)
x7 <- x3 + x4 + 2.5 * runif(n)
x8 <- x7 + 4.0 * rnorm(n)
# may produce class imbalance by changing '> 0' to something else
x9 <- (ifelse(x8 > 0, 1, 0))

# Fri May 12 21:55:50 2023 ------------------------------
# True relationship
lp <- x1 + x2 + .5 * x3 + .4 * x7 + 1.2 * x9
y <- lp + sigma * rnorm(n)

# model frame
df <- data.frame(y, x1, x2, x3, x4,
           x5, x6, x7, x8, x9)
df$x9 <- as.factor(df$x9)

# clean
rm(x1, x2, x3, x4, x5, x6, x7, x8, x9, y, sigma, n, lp)
# Sun May 14 01:00:15 2023 ------------------------------

#TRUE RELATIONSHIP x1, x2, x3, x7, x9
```



# How bad is it?
```{r}
par(mfrow = c(2, 2))
plot(lm(y ~ ., data = df))
summary(lm(y ~ ., data = df))
```


# Split data
```{r, split}
# re-produce
set.seed(2) 
# train/test split.
df <- initial_split(df, prop = 3/4) # split using 3/4 of data for 
# trainig and eval on rest.
df_train <- training(df) # assign training data to own data frame
df_test <- testing(df) # assign test data to own data frame
```


## Fold data
```{r}
set.seed(3)
# prepare cross-validation, 6-folds.
df_folds <- vfold_cv(df_train, v = 5) 
# regarding repeats, if the baseline value of σ is impractically large, 
# the diminishing returns on replication may still be worth the extra 
# computational costs. This dataset is small so I might aswell...
df_folds
```



### random forest - against overfiting(bagging)
```{r}

# Define recipe
rf_rec <- recipe(y ~., data = df_train) %>% # formula
    #   step_nzv(all_predictors()) %>% # pre-proc
    #   step_corr(all_numeric(), -all_outcomes()) %>%
    #   step_lincomb(all_outcomes(), -all_outcomes()) %>%
   #     step_interact(terms = y ~ .^2, sep = ":")  %>% # All 2-way interactions
    #   step_normalize(all_numeric(), - all_outcomes()) %>%
    #   step_normalize() %>%
    #   step_pca(all_predictors(), num_comp = 6)
        step_dummy(x9)# defnie dummy 


# See predictors
#rf_rec %>% prep() %>% juice()
      

# Random forest, ensemble of Decision Trees, usually trained with the "bagging" method. 
# The general idea of bagging is to create several subsets of data from the training 
# sample, chosen randomly with replacement, and train a separate Decision Tree on each. 
# The final prediction is then the most frequent prediction (mode) of the collection 
# of Trees.

# Sun May 14 15:27:56 2023 ------------------------------

rf_mdl <- rand_forest(min_n = tune(), 
                      trees = tune(), 
                      mtry = tune()) %>%
    set_mode("regression") %>%
    set_engine("ranger", importance = "impurity")

# Sun May 14 15:27:59 2023 ------------------------------

rf_grid <- grid_regular(
    min_n(range = c(2, 10)),
    trees((range = c(3, 10))),
    mtry(range = c(1, 8)),
    levels = 5)


#
rf_grid <- grid_regular(
  extract_parameter_set_dials(rf_mdl) %>% 
    finalize(df_folds),
  levels = 5)

# min_n (or min_samples_split): This parameter controls the minimum number of samples required to split an internal node. Larger values can help prevent overfitting but may lead to underfitting if set too high. For small datasets, you can start with a range of 2-10. For larger datasets, it might be more appropriate to use higher values, like 10-100, or even higher.

# Sun May 14 15:31:30 2023 ------------------------------

# trees (or n_estimators): This is the number of trees in the forest. More trees typically lead to better results but can increase training time. A common range is 100 to 1000, but you can also try larger values if your computational resources allow it. Note that after a certain point, adding more trees might not significantly improve model performance.

# Sun May 14 15:31:34 2023 ------------------------------

# tree_depth (or max_depth): This parameter controls the maximum depth of a tree. Deeper trees can learn more specific patterns but are more prone to overfitting. You can try a range of values, such as 10-30, or even use None (the default) to allow unlimited depth. It's essential to test different values and see which works best for your specific problem.

# Sun May 14 15:31:37 2023 ------------------------------

# mtry (or max_features): This parameter controls the number of features to consider when looking for the best split. It helps introduce randomness and decorrelate the trees in the ensemble. Common choices are: 
# "sqrt": The square root of the total number of features.
# "log2": The base-2 logarithm of the total number of features.
# 0.5 or some other proportion: A fixed proportion of the total number of features.
# None: Use all features.

# Sun May 14 15:31:40 2023 ------------------------------


# stating that i want to save the predictions made from tuning process,
# save the individual workflows.

mdl_control <- control_grid(save_pred = TRUE, 
                              save_workflow = TRUE)

# Sun May 14 01:22:17 2023 ------------------------------

# basically stating which metrics I need for comparison
mdl_metrics_reg <- metric_set(rmse, mae, rsq, mape)
mdl_metrics_class <- metric_set(roc_auc, sens, spec, mn_log_loss)

# Sun May 14 01:23:30 2023 ------------------------------

# define workflow tying together model statement and preproc recipe
rf_wf <- workflow() %>%
    add_model(rf_mdl) %>%
    add_recipe(rf_rec)

# Sun May 14 01:25:00 2023 ------------------------------


# Mutiple cores, smoke 'em if you(still) got em.
#v <- parallel::detectCores() # your number of lcores
#cl <- parallel::makeCluster(v) # or replace with your number of lcores.
doParallel::registerDoParallel()

# Fri May 12 15:25:21 2023 ------------------------------
# Tuning is the process of choosing the most optimal hyperparameters, 
# the goal being to optimize the predictive performance. 
# A hyperparameter tuning  grid is a table of values, that are specified. 
# During the tuning process, a separate model is trained for each combination 
# of  hyperparameters in the tuning grid. The performance is evaluated, using 
# cross-validation. The hyperparameter values that result in the model with the
# best # performance are then chosen as the final hyperparameters.

set.seed(82)

# time will depend on n parameters to tune, size of data, e.tc
# Is there a way to determine on beforehand how long time it will take?

tune_rf <- tune_grid(
    rf_wf,
    resamples = df_folds,
    grid = rf_grid,
    metrics = mdl_metrics_reg,
    control = mdl_control)

# we won't need paralell moving forward.
doParallel::stopImplicitCluster()

# Sun May 14 01:29:00 2023 ------------------------------
# find best model parameters from tuning process using metric, rmse
rf_best <- tune_rf %>% show_best("rmse") %>%
    head(1) %>%
    select(mtry:min_n)
rf_best

# Sun May 14 01:30:20 2023 ------------------------------
# finalize sub-models to enable comparisons between each and all models. last_fit()
# emulates the process where, after determining the best model, the final fit on 
# the entire training set is needed and is then evaluated on the initially held-out
# test set. Note that this is the first time the models get to see the held-out
# data set.

rf_fin <- rf_wf %>%
    finalize_workflow(rf_best) %>%
    last_fit(df)

# see final metrics, eval on test set
rf_fin %>% collect_metrics()

# predicted vs observed
rf_fin %>%
    collect_predictions() %>%
    ggplot(aes(x = y, y = .pred)) +
    geom_abline(color = "gray50", lty = 2) +
    geom_point(alpha = 0.3) +
    geom_smooth(se = T, lwd = 0.5, alpha = 0.6, method = "loess", 
                fill = "lightgrey") +
    theme_bw() +
    coord_obs_pred() +
    labs(x = "observed", y = "predicted")

# variable importance
rf_fin %>% extract_fit_parsnip() %>% 
  vip(geom = "point") + 
  labs(title = "#TRUE RELATIONSHIP x1, x2, x3, x7, x9") + 
  theme_bw()
```

